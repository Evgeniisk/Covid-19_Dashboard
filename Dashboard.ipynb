{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By Evgenii Korostelev, Student ID number: 200251127"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COVID-19 booster vaccine and COVID-19 disease dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a COVID-19 booster vaccine vaccine waves analysis against COVID-19 cases, deaths and hospital admissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import ipywidgets as wdg\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# make figures larger\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON files and store the raw data in some variable. Edit as appropriate\n",
    "with open(\"cases.json\", \"rt\") as INFILE:\n",
    "    cases=json.load(INFILE)\n",
    "with open(\"admissions.json\", \"rt\") as INFILE:\n",
    "    admissions=json.load(INFILE)\n",
    "with open(\"deaths.json\", \"rt\") as INFILE:\n",
    "    deaths=json.load(INFILE)\n",
    "with open(\"autumn22_doses.json\", \"rt\") as INFILE:\n",
    "    autumn22_doses=json.load(INFILE)\n",
    "with open(\"spring23_doses.json\", \"rt\") as INFILE:\n",
    "    spring23_doses=json.load(INFILE)\n",
    "with open(\"autumn23_doses.json\", \"rt\") as INFILE:\n",
    "    autumn23_doses=json.load(INFILE)\n",
    "with open(\"spring24_doses.json\", \"rt\") as INFILE:\n",
    "    spring24_doses=json.load(INFILE)\n",
    "with open(\"autumn24_doses.json\", \"rt\") as INFILE:\n",
    "    autumn24_doses=json.load(INFILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wangle_data():\n",
    "    #below is the data aggregation algorithm which i designed to use for aggregating data in booster vaccine .json files:\n",
    "    #(Although it could be used to aggregate data in other files in my opinion, I think I designed it quite well for it to be able to be applied\n",
    "    #to other data in .json files. It is an algorithm I designed to be quite broad in its applicability.\n",
    "    def aggregate_data_in_json(file_path, match_keys, other_keys, sum_keys, match_constraints=None):\n",
    "        #Parameters: \n",
    "            #file_path (str): Path to the JSON file containing a list of dictionaries.\n",
    "            #match_key (list): List of keys to compare for equality between datasets.\n",
    "            #other_keys(list): List of other keys to include in the aggregated result.\n",
    "            #sum_keys (list): List of keys whose values need to be aggregated.\n",
    "            #match_constraints (dict, optional): A dictionary of key-value pairs to use as filtering constraints.\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "    \n",
    "        if match_constraints:\n",
    "            data = [\n",
    "                item for item in data\n",
    "                if all(item.get(key) == value for key, value in match_constraints.items())\n",
    "            ]\n",
    "            \n",
    "        aggregated_result = []\n",
    "        processed_key_combinations = set()\n",
    "        \n",
    "        for item in data:\n",
    "            key_combination = tuple(item.get(key) for key in match_keys)\n",
    "            if key_combination in processed_key_combinations:\n",
    "                continue\n",
    "                \n",
    "    \n",
    "            aggregated_item = {key: item.get(key) for key in match_keys}\n",
    "            for key in other_keys:\n",
    "                aggregated_item[key] = item.get(key)\n",
    "    \n",
    "    \n",
    "            for key in sum_keys:\n",
    "                aggregated_item[key] = item.get(key, 0)\n",
    "            \n",
    "            for other_item in data:\n",
    "                other_key_combination = tuple(other_item.get(key) for key in match_keys)\n",
    "                if other_key_combination == key_combination and other_item != item:\n",
    "                    for key in sum_keys:\n",
    "                        aggregated_item[key] += other_item.get(key, 0)\n",
    "            aggregated_result.append(aggregated_item)\n",
    "            processed_key_combinations.add(key_combination)\n",
    "        return aggregated_result\n",
    "    \n",
    "    \n",
    "    #Here I utilised the aggregate_data_in_json function to aggregate all the required data.\n",
    "    #I also created seperate files for aggregated data.\n",
    "    #Please also note that I specified different constraints for different files because there was a lot of duplicate data I figured which constraints\n",
    "    #to use by looking at the data inside json files - how it is structured. (Please have a look inside some of those files and see for yourself).\n",
    "    file_path = \"autumn22_doses.json\"\n",
    "    match_keys = [\"date\"]\n",
    "    other_keys = [\"metric\"]\n",
    "    sum_keys = [\"metric_value\"]\n",
    "    match_constraints = {\"sex\": \"all\", \"age\": \"50+\"}\n",
    "    \n",
    "    result = aggregate_data_in_json(file_path, match_keys, other_keys, sum_keys, match_constraints)\n",
    "    with open(\"aggregated_\"+file_path, 'w') as aggregated_file:\n",
    "        json.dump(result, aggregated_file)\n",
    "    \n",
    "    \n",
    "    file_path = \"spring23_doses.json\"\n",
    "    match_keys = [\"date\"]\n",
    "    other_keys = [\"metric\"]\n",
    "    sum_keys = [\"metric_value\"]\n",
    "    match_constraints = {\"sex\": \"all\", \"age\": \"75+\"}\n",
    "    \n",
    "    result = aggregate_data_in_json(file_path, match_keys, other_keys, sum_keys, match_constraints)\n",
    "    with open(\"aggregated_\"+file_path, 'w') as aggregated_file:\n",
    "        json.dump(result, aggregated_file)\n",
    "    \n",
    "    \n",
    "    file_path = \"autumn23_doses.json\"\n",
    "    match_keys = [\"date\"]\n",
    "    other_keys = [\"metric\"]\n",
    "    sum_keys = [\"metric_value\"]\n",
    "    \n",
    "    match_constraints = {\"sex\": \"all\", \"age\": \"65+\"}\n",
    "    \n",
    "    \n",
    "    result = aggregate_data_in_json(file_path, match_keys, other_keys, sum_keys, match_constraints)\n",
    "    with open(\"aggregated_\"+file_path, 'w') as aggregated_file:\n",
    "        json.dump(result, aggregated_file)\n",
    "    \n",
    "    \n",
    "    file_path = \"spring24_doses.json\"\n",
    "    match_keys = [\"date\"]\n",
    "    other_keys = [\"metric\"]\n",
    "    sum_keys = [\"metric_value\"]\n",
    "    match_constraints = {\"sex\": \"all\", \"age\": \"75+\"}\n",
    "    \n",
    "    result = aggregate_data_in_json(file_path, match_keys, other_keys, sum_keys, match_constraints)\n",
    "    with open(\"aggregated_\"+file_path, 'w') as aggregated_file:\n",
    "        json.dump(result, aggregated_file)\n",
    "    \n",
    "    \n",
    "    file_path = \"autumn24_doses.json\"\n",
    "    match_keys = [\"date\"]\n",
    "    other_keys = [\"metric\"]\n",
    "    sum_keys = [\"metric_value\"]\n",
    "    match_constraints = {\"sex\": \"all\", \"age\": \"65+\"}\n",
    "    \n",
    "    result = aggregate_data_in_json(file_path, match_keys, other_keys, sum_keys, match_constraints)\n",
    "    with open(\"aggregated_\"+file_path, 'w') as aggregated_file:\n",
    "        json.dump(result, aggregated_file)\n",
    "    \n",
    "    \n",
    "    with open(\"aggregated_autumn22_doses.json\", \"rt\") as INFILE:\n",
    "        aggregated_autumn22_doses=json.load(INFILE)\n",
    "    with open(\"aggregated_spring23_doses.json\", \"rt\") as INFILE:\n",
    "        aggregated_spring23_doses=json.load(INFILE)\n",
    "    with open(\"aggregated_autumn23_doses.json\", \"rt\") as INFILE:\n",
    "        aggregated_autumn23_doses=json.load(INFILE)\n",
    "    with open(\"aggregated_spring24_doses.json\", \"rt\") as INFILE:\n",
    "        aggregated_spring24_doses=json.load(INFILE)\n",
    "    with open(\"aggregated_autumn24_doses.json\", \"rt\") as INFILE:\n",
    "        aggregated_autumn24_doses=json.load(INFILE)\n",
    "\n",
    "    #Data mugging:\n",
    "    data={}\n",
    "    for dataset in [admissions, cases, deaths, aggregated_autumn22_doses, aggregated_spring23_doses, aggregated_autumn23_doses, aggregated_spring24_doses, aggregated_autumn24_doses]:\n",
    "        for entry in dataset:\n",
    "            date=entry['date']\n",
    "            metric=entry['metric']\n",
    "            value=entry['metric_value']\n",
    "            if date not in data:\n",
    "                data[date]={}\n",
    "            data[date][metric]=value\n",
    "\n",
    "    dates=list(data.keys())\n",
    "    dates.sort()\n",
    "\n",
    "    def parse_date(datestring):\n",
    "        \"\"\" Convert a date string into a pandas datetime object \"\"\"\n",
    "        return pd.to_datetime(datestring, format=\"%Y-%m-%d\")\n",
    "\n",
    "    startdate=parse_date(dates[0])\n",
    "    enddate=parse_date(dates[-1])\n",
    "\n",
    "    index=pd.date_range(startdate, enddate, freq='D')\n",
    "    timeseriesdf=pd.DataFrame(index=index, columns=['cases', 'admissions', 'deaths', 'aggregated_autumn22_doses', 'aggregated_spring23_doses', 'aggregated_autumn23_doses', 'aggregated_spring24_doses', 'aggregated_autumn24_doses'])\n",
    "\n",
    "    # translate the columns to our metrics\n",
    "    metrics ={'cases': 'COVID-19_cases_casesByDay',\n",
    "              'admissions': 'COVID-19_healthcare_admissionByDay',\n",
    "              'deaths': 'COVID-19_deaths_ONSByDay',\n",
    "              'aggregated_autumn22_doses': 'COVID-19_vaccinations_autumn22_dosesByDay', \n",
    "              'aggregated_spring23_doses': 'COVID-19_vaccinations_spring23_dosesByDay', \n",
    "              'aggregated_autumn23_doses': 'COVID-19_vaccinations_autumn23_dosesByDay', \n",
    "              'aggregated_spring24_doses': 'COVID-19_vaccinations_spring24_dosesByDay', \n",
    "              'aggregated_autumn24_doses': 'COVID-19_vaccinations_autumn24_dosesByDay'}\n",
    "\n",
    "    for date, entry in data.items(): # each entry is a dictionary with cases, admissions and deaths\n",
    "        pd_date=parse_date(date) # convert to Pandas format\n",
    "        for column in ['cases', 'admissions', 'deaths', 'aggregated_autumn22_doses', 'aggregated_spring23_doses', 'aggregated_autumn23_doses', 'aggregated_spring24_doses', 'aggregated_autumn24_doses']: \n",
    "            metric_name=metrics[column]\n",
    "            # Not assuming all values are there for every date - if a value is not available, insert a 0.0\n",
    "            value= entry.get(metric_name, 0.0)\n",
    "            # this is the way I access a specific location in the dataframe - by using .loc\n",
    "            # and put index,column in a single set of [ ]\n",
    "            timeseriesdf.loc[date, column]=value\n",
    "            \n",
    "    # fill in any remaining \"holes\" due to missing dates\n",
    "    timeseriesdf.fillna(0.0, inplace=True)\n",
    "            \n",
    "    return timeseriesdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = wangle_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download current data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click the button below to update the data for plotting.\n",
    "If the there is an error in updating the data for plotting, you will be notified of this after clicking the button and the currently stored data for graph plotting will remain the same and will not be overwritten.\n",
    "\n",
    "Note: After you refresh the data, graphs will not update until you, the user interact with a widget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class APIwrapper:\n",
    "    # class variables shared among all instances\n",
    "    _access_point=\"https://api.ukhsa-dashboard.data.gov.uk\"\n",
    "    _last_access=0.0 # time of last api access\n",
    "    \n",
    "    def __init__(self, theme, sub_theme, topic, geography_type, geography, metric):\n",
    "        \"\"\" Init the APIwrapper object, constructing the endpoint from the structure\n",
    "        parameters \"\"\"\n",
    "        # build the path with all the required structure parameters. You do not need to edit this line,\n",
    "        # parameters will be replaced by the actual values when you instantiate an object of the class!\n",
    "        url_path=(f\"/themes/{theme}/sub_themes/{sub_theme}/topics/{topic}/geography_types/\" +\n",
    "                  f\"{geography_type}/geographies/{geography}/metrics/{metric}\")\n",
    "        # our starting API endpoint\n",
    "        self._start_url=APIwrapper._access_point+url_path\n",
    "        self._filters=None\n",
    "        self._page_size=-1\n",
    "        # will contain the number of items\n",
    "        self.count=None\n",
    "\n",
    "    def get_page(self, filters={}, page_size=5):\n",
    "        \"\"\" Access the API and download the next page of data. Sets the count\n",
    "        attribute to the total number of items available for this query. Changing\n",
    "        filters or page_size will cause get_page to restart from page 1. Rate\n",
    "        limited to three request per second. The page_size parameter sets the number\n",
    "        of data points in one response page (maximum 365); use the default value \n",
    "        for debugging your structure and filters. \"\"\"\n",
    "        # Check page size is within range\n",
    "        if page_size>365:\n",
    "            raise ValueError(\"Max supported page size is 365\")\n",
    "        # restart from first page if page or filters have changed\n",
    "        if filters!=self._filters or page_size!=self._page_size:\n",
    "            self._filters=filters\n",
    "\n",
    "            self._page_size=page_size\n",
    "            self._next_url=self._start_url\n",
    "        # signal the end of data condition\n",
    "        if self._next_url==None: \n",
    "            return [] # we already fetched the last page\n",
    "        # simple rate limiting to avoid bans\n",
    "        curr_time=time.time() # Unix time: number of seconds since the Epoch\n",
    "        deltat=curr_time-APIwrapper._last_access\n",
    "        if deltat<0.33: # max 3 requests/second\n",
    "            time.sleep(0.33-deltat)\n",
    "        APIwrapper._last_access=curr_time\n",
    "        # build parameter dictionary by removing all the None\n",
    "        # values from filters and adding page_size\n",
    "        parameters={x: y for x, y in filters.items() if y!=None}\n",
    "        parameters['page_size']=page_size\n",
    "        # the page parameter is already included in _next_url.\n",
    "        # This is the API access. Response is a dictionary with various keys.\n",
    "        # the .json() method decodes the response into Python object (dictionaries,\n",
    "        # lists; 'null' values are translated as None).\n",
    "        response = requests.get(self._next_url, params=parameters).json()\n",
    "        # update url so we'll fetch the next page\n",
    "        self._next_url=response['next']\n",
    "        self.count=response['count']\n",
    "        # data are in the nested 'results' list\n",
    "        return response['results']\n",
    "\n",
    "    def get_all_pages(self, filters={}, page_size=365):\n",
    "        \"\"\" Access the API and download all available data pages of data. Sets the count\n",
    "        attribute to the total number of items available for this query. API access rate\n",
    "        limited to three request per second. The page_size parameter sets the number\n",
    "        of data points in one response page (maximum 365), and controls the trade-off\n",
    "        between time to load a page and number of pages; the default should work well \n",
    "        in most cases. The number of items returned should in any case be equal to \n",
    "        the count attribute. \"\"\"\n",
    "        data=[] # build up all data here\n",
    "        while True:\n",
    "            # use get_page to do the job, including the pacing\n",
    "            next_page=self.get_page(filters, page_size)\n",
    "            if next_page==[]:\n",
    "                break # we are done\n",
    "            data.extend(next_page)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "structure={\"theme\": \"infectious_disease\", \n",
    "           \"sub_theme\": \"respiratory\",\n",
    "           \"topic\": \"COVID-19\",\n",
    "           \"geography_type\": \"Nation\", \n",
    "           \"geography\": \"England\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to clear files.\n",
    "def clear_files(filenames):\n",
    "        \"\"\"Deletes the specified files if they exist or initializes them empty.\"\"\"\n",
    "        for filename in filenames:\n",
    "            with open(filename, 'w') as file:\n",
    "                json.dump([], file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c33be96519548e28059d8e1ef261248",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Refresh data', icon='download', style=ButtonStyle(), tooltip='Click to download current Pu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# our API access function. This will be called by the button when it is clicked\n",
    "try:\n",
    "    def access_api(button):\n",
    "    # Ignore the button parameter\n",
    "    #\n",
    "    # put code for polling the API here\n",
    "    \n",
    "    # Defining all the file names that need clearing\n",
    "        filenames = [\n",
    "            \"cases.json\",\n",
    "            \"admissions.json\",\n",
    "            \"deaths.json\",\n",
    "            \"autumn22_doses.json\",\n",
    "            \"spring23_doses.json\",\n",
    "            \"autumn23_doses.json\",\n",
    "            \"spring24_doses.json\",\n",
    "            \"autumn24_doses.json\",\n",
    "        ]\n",
    "\n",
    "    # Clearing files before processing\n",
    "        clear_files(filenames)\n",
    "\n",
    "\n",
    "    \n",
    "        structure[\"metric\"]=\"COVID-19_cases_casesByDay\"\n",
    "        api=APIwrapper(**structure)\n",
    "        cases=api.get_all_pages()\n",
    "        print(f\"Data points expected: {api.count}\")\n",
    "        print(f\"Data points retrieved: {len(cases)}\")\n",
    "\n",
    "        structure[\"metric\"]=\"COVID-19_healthcare_admissionByDay\"\n",
    "        api=APIwrapper(**structure)\n",
    "        admissions=api.get_all_pages()\n",
    "        print(f\"Data points expected: {api.count}\")\n",
    "        print(f\"Data points retrieved: {len(admissions)}\")\n",
    "\n",
    "        structure[\"metric\"]=\"COVID-19_deaths_ONSByDay\"\n",
    "        api=APIwrapper(**structure)\n",
    "        deaths=api.get_all_pages()\n",
    "        print(f\"Data points expected: {api.count}\")\n",
    "\n",
    "        print(f\"Data points retrieved: {len(deaths)}\")\n",
    "\n",
    "        structure[\"metric\"]=\"COVID-19_vaccinations_autumn22_dosesByDay\"\n",
    "        api=APIwrapper(**structure)\n",
    "        autumn22_doses=api.get_all_pages()\n",
    "        print(f\"Data points expected: {api.count}\")\n",
    "        print(f\"Data points retrieved: {len(autumn22_doses)}\")\n",
    "\n",
    "        structure[\"metric\"]=\"COVID-19_vaccinations_spring23_dosesByDay\"\n",
    "        api=APIwrapper(**structure)\n",
    "        spring23_doses=api.get_all_pages()\n",
    "        print(f\"Data points expected: {api.count}\")\n",
    "        print(f\"Data points retrieved: {len(spring23_doses)}\")\n",
    "\n",
    "        structure[\"metric\"]=\"COVID-19_vaccinations_autumn23_dosesByDay\"\n",
    "        api=APIwrapper(**structure)\n",
    "        autumn23_doses=api.get_all_pages()\n",
    "        print(f\"Data points expected: {api.count}\")\n",
    "        print(f\"Data points retrieved: {len(autumn23_doses)}\")\n",
    "\n",
    "        structure[\"metric\"]=\"COVID-19_vaccinations_spring24_dosesByDay\"\n",
    "        api=APIwrapper(**structure)\n",
    "        spring24_doses=api.get_all_pages()\n",
    "        print(f\"Data points expected: {api.count}\")\n",
    "        print(f\"Data points retrieved: {len(spring24_doses)}\")\n",
    "\n",
    "        structure[\"metric\"]=\"COVID-19_vaccinations_autumn24_dosesByDay\"\n",
    "        api=APIwrapper(**structure)\n",
    "        autumn24_doses=api.get_all_pages()\n",
    "        print(f\"Data points expected: {api.count}\")\n",
    "        print(f\"Data points retrieved: {len(autumn24_doses)}\")\n",
    "\n",
    "    \n",
    "    \n",
    "        with open(\"cases.json\", \"wt\") as OUTF:\n",
    "            json.dump(cases, OUTF)\n",
    "    \n",
    "        with open(\"admissions.json\", \"wt\") as OUTF:\n",
    "            json.dump(admissions, OUTF)\n",
    "\n",
    "        with open(\"deaths.json\", \"wt\") as OUTF:\n",
    "            json.dump(deaths, OUTF)\n",
    "\n",
    "        with open(\"autumn22_doses.json\", \"wt\") as OUTF:\n",
    "            json.dump(autumn22_doses, OUTF)\n",
    "\n",
    "        with open(\"spring23_doses.json\", \"wt\") as OUTF:\n",
    "            json.dump(spring23_doses, OUTF)\n",
    "\n",
    "        with open(\"autumn23_doses.json\", \"wt\") as OUTF:\n",
    "            json.dump(autumn23_doses, OUTF)\n",
    "\n",
    "        with open(\"spring24_doses.json\", \"wt\") as OUTF:\n",
    "            json.dump(spring24_doses, OUTF)\n",
    "\n",
    "        with open(\"autumn24_doses.json\", \"wt\") as OUTF:\n",
    "            json.dump(autumn24_doses, OUTF)\n",
    "\n",
    "\n",
    "        with open(\"cases.json\", \"rt\") as INFILE:\n",
    "            cases=json.load(INFILE)\n",
    "        with open(\"admissions.json\", \"rt\") as INFILE:\n",
    "            admissions=json.load(INFILE)\n",
    "        with open(\"deaths.json\", \"rt\") as INFILE:\n",
    "            deaths=json.load(INFILE)\n",
    "        with open(\"autumn22_doses.json\", \"rt\") as INFILE:\n",
    "            autumn22_doses=json.load(INFILE)\n",
    "        with open(\"spring23_doses.json\", \"rt\") as INFILE:\n",
    "            spring23_doses=json.load(INFILE)\n",
    "        with open(\"autumn23_doses.json\", \"rt\") as INFILE:\n",
    "            autumn23_doses=json.load(INFILE)\n",
    "        with open(\"spring24_doses.json\", \"rt\") as INFILE:\n",
    "            spring24_doses=json.load(INFILE)\n",
    "        with open(\"autumn24_doses.json\", \"rt\") as INFILE:\n",
    "            autumn24_doses=json.load(INFILE)\n",
    "\n",
    "\n",
    "        global df\n",
    "        df=wangle_data()\n",
    "    \n",
    "    # after all is done, I can switch the icon on the button to a \"check\" sign\n",
    "    # and optionally disable the button - it won't be needed again.\n",
    "        apibutton.icon=\"check\"\n",
    "        apibutton.disabled=True\n",
    "\n",
    "# see the doc for the parameters    \n",
    "    apibutton=wdg.Button(\n",
    "        description='Refresh data',\n",
    "        disabled=False,\n",
    "\n",
    "        button_style='',\n",
    "        tooltip='Click to download current Public Health England data',\n",
    "        icon='download'\n",
    "    )\n",
    "\n",
    "# registered the callback function with the button:\n",
    "    apibutton.on_click(access_api)\n",
    "\n",
    "# this is an iPython function that generalises print for Jupyter Notebooks; I used it to \n",
    "# display the widgets:\n",
    "    display(apibutton)\n",
    "except:\n",
    "    print(\"There was an error when refershing your data, please be notified that the current data has not been overwritten\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph with interactive controls shows metrics of daily Covid-19 cases, admissions, deaths, and booster vaccine dose administrations for respective booster vaccine waves.\n",
    "The aggregated_..._doses show figures for total daily booster vaccine does for each COVID-19 booster vaccine wave according to its broadly defined time period.\n",
    "This graph can be viewed with all data stacked on each other in one graph on one timeline and individually for each metric.\n",
    "I recommend to view the data on a linear scale to compare the magnitudes of COVID-19 cases, deaths and hospital admissions with COVID-19 booster vaccine waves of administration.\n",
    "I also recommend to view the data on a log scale to compare the effectiveness of COVID-19 waves against COVID-19 cases, deaths and hospital admissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db1cb1c83b9543af922d059ca74f8492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(SelectMultiple(description='Stats:', index=(1, 2, 3, 4, 5, 6, 7, 8), options=('all', 'cases', '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ded3087bb94448ddaea77027573d611b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "series=wdg.SelectMultiple(\n",
    "    options=['all', 'cases', 'admissions', 'deaths', 'aggregated_autumn22_doses', 'aggregated_spring23_doses', 'aggregated_autumn23_doses', 'aggregated_spring24_doses', 'aggregated_autumn24_doses'],\n",
    "    value=['cases', 'admissions', 'deaths', 'aggregated_autumn22_doses', 'aggregated_spring23_doses', 'aggregated_autumn23_doses', 'aggregated_spring24_doses', 'aggregated_autumn24_doses'],\n",
    "    rows=9,\n",
    "    description='Stats:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "scale=wdg.RadioButtons(\n",
    "    options=['linear', 'log'],\n",
    "    description='Scale:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# I think VBox looks nicer than Hbox\n",
    "controls=wdg.VBox([series, scale])\n",
    "\n",
    "def timeseries_graph(gcols, gscale):\n",
    "    logscale = gscale == 'log'  # Determine if log scale is selected\n",
    "    \n",
    "    # Check if \"all\" is in the selected options\n",
    "    if 'all' in gcols:\n",
    "        # Plot all columns and reset figure to avoid overlaps\n",
    "  \n",
    "        plt.figure()\n",
    "        df.plot(logy=logscale, figsize=(12,7))\n",
    "        plt.title(\"Log Scale\" if logscale else \"Linear Scale\")\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.ylabel(\"Metric\")\n",
    "        plt.legend(loc='upper left')\n",
    "        plt.show()\n",
    "    else:\n",
    "        ncols = len(gcols)\n",
    "        if ncols > 0:\n",
    "            # Plot selected columns\n",
    "            plt.figure()\n",
    "            df[list(gcols)].plot(logy=logscale, figsize=(12,7))\n",
    "            plt.title(\"Log Scale\" if logscale else \"Linear Scale\")\n",
    "            plt.xlabel(\"Date\")\n",
    "            plt.ylabel(\"Metric\")\n",
    "            plt.legend(loc='upper left')\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"Click to select data for graph\")\n",
    "            print(\"(CTRL-Click to select more than one category)\")\n",
    "\n",
    "graph = wdg.interactive_output(timeseries_graph, {'gcols': series, 'gscale': scale})\n",
    "\n",
    "# Display controls and graph\n",
    "display(controls, graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author and License** \"Based on UK Government [data](https://ukhsa-dashboard.data.gov.uk/) published by the [UK Health Security Agency](https://www.gov.uk/government/organisations/uk-health-security-agency) and on the [DIY Disease Tracking Dashboard Kit](https://github.com/fsmeraldi/diy-covid19dash) by Evgenii Korostelev. Released under the [GNU GPLv3.0 or later](https://www.gnu.org/licenses/).\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COVID-19Dashboard",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
